{
  "paragraphs": [
    {
      "text": "print(s\"\"\"%html\n<center>\n    <h1><a href=\"http://diplodatos.famaf.unc.edu.ar/\">Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones</a></h1>\n    <h2>Curso <a href=\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\">Programación Distribuida sobre Grandes Volúmenes de Datos</a></h2>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\"> Damián Barsotti  </h3>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"$baseDir/comun/logo%20UNC%20FAMAF%202016.png\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n\"\"\")\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "fontSize": 9,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<center>\n    <h1><a href=\"http://diplodatos.famaf.unc.edu.ar/\">Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones</a></h1>\n    <h2>Curso <a href=\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\">Programación Distribuida sobre Grandes Volúmenes de Datos</a></h2>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\"> Damián Barsotti  </h3>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/comun/logo%20UNC%20FAMAF%202016.png\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632520_1402976436",
      "id": "20160720-131940_474698556",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "focus": true,
      "$$hashKey": "object:2161"
    },
    {
      "text": "%md\n# Introducción a Spark\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h1>Introducción a Spark</h1>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632522_568980290",
      "id": "20160628-160644_98292392",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2162"
    },
    {
      "text": "%md\n## Características\n\n### 100x más rápido que Hadoop MapReduce en memoria.\n### 10x más rápido en disco.\n  ![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_speed.png)\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Características</h2>\n<h3>100x más rápido que Hadoop MapReduce en memoria.</h3>\n<h3>10x más rápido en disco.</h3>\n<p><img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_speed.png\" /></p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632522_781140655",
      "id": "20171013-102503_1459120534",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2163"
    },
    {
      "text": "%md\n### Multiplataforma\n\n* Corre en Hadoop Yarn, Mesos, standalone o en la nube (AWS, Azure, ...)\n* Acceso a datos en HDFS, Cassandra, HBase, Hive, Tachyon, JDBC, etc.\n![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_multi_plataforma.png)\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Multiplataforma</h3>\n<ul>\n  <li>Corre en Hadoop Yarn, Mesos, standalone o en la nube (AWS, Azure, &hellip;)</li>\n  <li>Acceso a datos en HDFS, Cassandra, HBase, Hive, Tachyon, JDBC, etc.<br/><img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_multi_plataforma.png\" /></li>\n</ul>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632522_1813734753",
      "id": "20171013-105605_1380652694",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2164"
    },
    {
      "text": "%md\n### +50 empresas.\n\n### +200 desarrolladores.\n![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spar_contribs.png)\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>+50 empresas.</h3>\n<h3>+200 desarrolladores.</h3>\n<p><img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spar_contribs.png\" /></p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632523_964417506",
      "id": "20171013-112527_2104876012",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2165"
    },
    {
      "title": "Múltiples funcionalidades en una plataforma (Stack unificado)",
      "text": "print(s\"\"\"%html\n<img src=\"$baseDir/01_intro_spark/unified_stack.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n\"\"\")\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/unified_stack.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632524_1835817207",
      "id": "20171013-110124_1830702370",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2166"
    },
    {
      "text": "%md\n## Fácil de usar\n\n* Interface de programación en Scala, Java, Python y R.\n* Notebooks: Zeppelin, Jupiter, ...",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:28:11+0000",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "fontSize": 9,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Fácil de usar</h2>\n<ul>\n  <li>Interface de programación en Scala, Java, Python y R.</li>\n  <li>Notebooks: Zeppelin, Jupiter, &hellip;</li>\n</ul>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632524_185413519",
      "id": "20171013-111851_1940139005",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2167"
    },
    {
      "title": "Word Count (MapReduce)",
      "text": "%md\n```java\npublic class WordCount {\n\n\tpublic static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\n\n\t\tprivate final static IntWritable one = new IntWritable(1);\n\n\t\tprivate Text word = new Text();\n\n\t\tpublic void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n\n\t\t\tString line = value.toString();\n\n\t\t\tStringTokenizer tokenizer = new StringTokenizer(line);\n\n\t\t\twhile (tokenizer.hasMoreTokens()) {\n\n\t\t\t\tword.set(tokenizer.nextToken());\n\n\t\t\t\toutput.collect(word, one);\n\n\t\t\t}\n\n\t\t}\n\n\t}\n\n\tpublic static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\n\n\t\tpublic void reduce(Text key, Iterator values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n\n\t\t\tint sum = 0;\n\n\t\t\twhile (values.hasNext()) {\n\n\t\t\t\tsum += values.next().get();\n\n\t\t\t}\n\n\t\t\toutput.collect(key, new IntWritable(sum));\n\n\t\t}\n\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tJobConf conf = new JobConf(WordCount.class);\n\n\t\tconf.setJobName(\"wordcount\");\n\n\t\tconf.setOutputKeyClass(Text.class);\n\n\t\tconf.setOutputValueClass(IntWritable.class);\n\n\t\tconf.setMapperClass(Map.class);\n\n\t\tconf.setCombinerClass(Reduce.class);\n\n\t\tconf.setReducerClass(Reduce.class);\n\n\t\tconf.setInputFormat(TextInputFormat.class);\n\n\t\tconf.setOutputFormat(TextOutputFormat.class);\n\n\t\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\n\n\t\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\n\n\t\tJobClient.runJob(conf);\n\n\t}\n\n}\n```\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<pre><code class=\"java\">public class WordCount {\n\n\tpublic static class Map extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {\n\n\t\tprivate final static IntWritable one = new IntWritable(1);\n\n\t\tprivate Text word = new Text();\n\n\t\tpublic void map(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {\n\n\t\t\tString line = value.toString();\n\n\t\t\tStringTokenizer tokenizer = new StringTokenizer(line);\n\n\t\t\twhile (tokenizer.hasMoreTokens()) {\n\n\t\t\t\tword.set(tokenizer.nextToken());\n\n\t\t\t\toutput.collect(word, one);\n\n\t\t\t}\n\n\t\t}\n\n\t}\n\n\tpublic static class Reduce extends MapReduceBase implements Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n\n\t\tpublic void reduce(Text key, Iterator values, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {\n\n\t\t\tint sum = 0;\n\n\t\t\twhile (values.hasNext()) {\n\n\t\t\t\tsum += values.next().get();\n\n\t\t\t}\n\n\t\t\toutput.collect(key, new IntWritable(sum));\n\n\t\t}\n\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tJobConf conf = new JobConf(WordCount.class);\n\n\t\tconf.setJobName(&quot;wordcount&quot;);\n\n\t\tconf.setOutputKeyClass(Text.class);\n\n\t\tconf.setOutputValueClass(IntWritable.class);\n\n\t\tconf.setMapperClass(Map.class);\n\n\t\tconf.setCombinerClass(Reduce.class);\n\n\t\tconf.setReducerClass(Reduce.class);\n\n\t\tconf.setInputFormat(TextInputFormat.class);\n\n\t\tconf.setOutputFormat(TextOutputFormat.class);\n\n\t\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\n\n\t\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\n\n\t\tJobClient.runJob(conf);\n\n\t}\n\n}\n</code></pre>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632524_2023573922",
      "id": "20171011-151944_1744659917",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2168"
    },
    {
      "title": "Word Count (Spark)",
      "text": "%pyspark\n\nlines = sc.textFile(\"README.md\")\n\nwords = lines \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .filter(lambda word: word)\n\n#MapReduce\nwordCount = words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:28:23+0000",
      "progress": 0,
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "fontSize": 14,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632524_1759891883",
      "id": "20201023-001936_119304475",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "dateStarted": "2021-10-21T21:28:23+0000",
      "dateFinished": "2021-10-21T21:29:07+0000",
      "status": "FINISHED",
      "$$hashKey": "object:2169"
    },
    {
      "title": "Un poco de Scala",
      "text": "%md\n\n* `lines` es un **array distribuido** de lineas de texto (`RDD[str]`).\n    - una parte del arreglo en cada **nodo del cluster**.\n\n* `lines` tiene el método `flatMap` (línea 6):\n    - `flatMap(lambda line: line.split(\" \"))` toma cada cada elemento del `RDD` (linea), lo convierte en sequencia de palabras y concatena estas secuencias:\n        - `lambda line: line.split(\" \")` es la **función** que toma una linea y la divide en una secuencia de palabras.\n        \n    - Su resultado es un array **distribuido** de palabras (`RDD[str]`).\n    \n* Al resultado de `flatMap` se aplica el método `filter` (línea 7):\n    - `filter(lambda word: word)` saca las palabras que son vacías (pueden aparecer?).\n    - `lambda word: word` es la **función** que pregunta si la palabra es vacía.\n    - `filter` devuelve un `RDD` que se almacena en `words`.\n\n* `words` tiene el método `map` (línea 11):\n    - `map(lambda word: (word,1))` agrega a cada palabra de `words` un `1`.\n    - El resultado es un **arreglo distribuido** de tuplas `RDD[(str, Int)]`.\n    \n* A este `RDD` se le aplica el método `reduceByKey` (línea 12):\n    - `reduceByKey(lambda n,m: n+m)` suma los `1`'s de las palabras iguales (la key es la palabra).\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "title": false,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<ul>\n  <li>\n    <p><code>lines</code> es un <strong>array distribuido</strong> de lineas de texto (<code>RDD[str]</code>).</p>\n    <ul>\n      <li>una parte del arreglo en cada <strong>nodo del cluster</strong>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><code>lines</code> tiene el método <code>flatMap</code> (línea 6):</p>\n    <ul>\n      <li>\n        <p><code>flatMap(lambda line: line.split(&quot; &quot;))</code> toma cada cada elemento del <code>RDD</code> (linea), lo convierte en sequencia de palabras y concatena estas secuencias:</p>\n        <ul>\n          <li><code>lambda line: line.split(&quot; &quot;)</code> es la <strong>función</strong> que toma una linea y la divide en una secuencia de palabras.</li>\n        </ul>\n      </li>\n      <li>\n      <p>Su resultado es un array <strong>distribuido</strong> de palabras (<code>RDD[str]</code>).</p></li>\n    </ul>\n  </li>\n  <li>\n    <p>Al resultado de <code>flatMap</code> se aplica el método <code>filter</code> (línea 7):</p>\n    <ul>\n      <li><code>filter(lambda word: word)</code> saca las palabras que son vacías (pueden aparecer?).</li>\n      <li><code>lambda word: word</code> es la <strong>función</strong> que pregunta si la palabra es vacía.</li>\n      <li><code>filter</code> devuelve un <code>RDD</code> que se almacena en <code>words</code>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><code>words</code> tiene el método <code>map</code> (línea 11):</p>\n    <ul>\n      <li><code>map(lambda word: (word,1))</code> agrega a cada palabra de <code>words</code> un <code>1</code>.</li>\n      <li>El resultado es un <strong>arreglo distribuido</strong> de tuplas <code>RDD[(str, Int)]</code>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>A este <code>RDD</code> se le aplica el método <code>reduceByKey</code> (línea 12):</p>\n    <ul>\n      <li><code>reduceByKey(lambda n,m: n+m)</code> suma los <code>1</code>&rsquo;s de las palabras iguales (la key es la palabra).</li>\n    </ul>\n  </li>\n</ul>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632525_958961776",
      "id": "20181010-120216_1622145406",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2170"
    },
    {
      "title": "Resultado Word Count Spark",
      "text": "%pyspark\n\nresult = wordCount \\\n    .sortBy((lambda p: p[1]), ascending = False) # ordena por cantidad\n\nlocal_result = result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print(word, count) # los imprimo\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:29:29+0000",
      "progress": 66,
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 14,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "from 4\nApache 3\nZeppelin 3\nand 3\nto 3\n* 2\n### 2\nbinary 2\nPlease 2\n[User 2\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://localhost:4040/jobs/job?id=0",
              "$$hashKey": "object:2657"
            },
            {
              "jobUrl": "http://localhost:4040/jobs/job?id=1",
              "$$hashKey": "object:2658"
            },
            {
              "jobUrl": "http://localhost:4040/jobs/job?id=2",
              "$$hashKey": "object:2659"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632528_1940845808",
      "id": "20171011-153126_91229243",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "dateStarted": "2021-10-21T21:29:29+0000",
      "dateFinished": "2021-10-21T21:29:35+0000",
      "status": "FINISHED",
      "$$hashKey": "object:2171"
    },
    {
      "title": "Run this",
      "text": "%pyspark\n\nuiHost = sc.getConf().get(\"spark.driver.host\")#.getOrElse(\"localhost\")\nuiPort = sc.uiWebUrl.split(\":\")[-1]\n\ntextNabuco = \"\"\"%html\nEjecutar esta celda.<br>\nHacer un tunel ssh a Nabuco:<br>\nssh -vCN -L 4040:localhost:{} -l &lt;tu login&gt; nabucodonosor.ccad.unc.edu.ar<br>\ny ver Spark UI en \n<a href=\"http://{}:{}\">http://{}(host):{}(port)</a>\n\"\"\".format(uiPort,\"localhost\",\"4040\",\"localhost\",\"4040\")\n\ntextLocal = \"\"\"%html\nEjecutar esta celda y ver Spark UI en \n<a href=\"http://{}:{}\">http://{}(host):{}(port)</a>\n\"\"\".format(uiHost,uiPort,uiHost,uiPort)\n\nif uiHost == \"200.16.29.165\":\n    print(textNabuco)\nelse:\n    print(textLocal)\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "editorHide": true,
        "fontSize": 15,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "Ejecutar esta celda y ver Spark UI en \n<a href=\"http://localhost:4040\">http://localhost(host):4040(port)</a>\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632528_473815496",
      "id": "20171010-193244_2031028749",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2172"
    },
    {
      "title": "Ejercicio 0 (word count)",
      "text": "%md\n* Crear una celda abajo de esta (poner mouse debajo de esta celda y seleccionar \"Add Paragraph\").\n* Copiar el programa `wordcount` anterior en la misma (esta en 2 celdas).\n    - [`shift`]-[`flechas`] para seleccionar.\n    - [`ctrl`]-[`c`] para copiar.\n    - [`ctrl`]-[`v`] para pegar.\n* Modificarlo para leer todas la lineas de los archivos en `./licenses/`\n    - Ayuda: si al método `textFile` se le indica el nombre de un directorio carga todos los archivo del mismo.\n* Ejecute la celda ([`shift`]-[`enter`])\n* Ver la cantidad de tareas en SparkUI\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<ul>\n  <li>Crear una celda abajo de esta (poner mouse debajo de esta celda y seleccionar &ldquo;Add Paragraph&rdquo;).</li>\n  <li>Copiar el programa <code>wordcount</code> anterior en la misma (esta en 2 celdas).\n    <ul>\n      <li>[<code>shift</code>]-[<code>flechas</code>] para seleccionar.</li>\n      <li>[<code>ctrl</code>]-[<code>c</code>] para copiar.</li>\n      <li>[<code>ctrl</code>]-[<code>v</code>] para pegar.</li>\n    </ul>\n  </li>\n  <li>Modificarlo para leer todas la lineas de los archivos en <code>./licenses/</code>\n    <ul>\n      <li>Ayuda: si al método <code>textFile</code> se le indica el nombre de un directorio carga todos los archivo del mismo.</li>\n    </ul>\n  </li>\n  <li>Ejecute la celda ([<code>shift</code>]-[<code>enter</code>])</li>\n  <li>Ver la cantidad de tareas en SparkUI</li>\n</ul>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632529_1753366693",
      "id": "20171010-205347_2087717007",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2173"
    },
    {
      "text": "%pyspark\n\nlines = sc.textFile(\"./licenses/\")\n\nwords = lines \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .filter(lambda word: word)\n\n#MapReduce\nwordCount = words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n\nresult = wordCount \\\n    .sortBy((lambda p: p[1]), ascending = False) # ordena por cantidad\n\nlocal_result = result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print(word, count) # los imprimo",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:32:10+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://localhost:4040/jobs/job?id=3",
              "$$hashKey": "object:2998"
            },
            {
              "jobUrl": "http://localhost:4040/jobs/job?id=4",
              "$$hashKey": "object:2999"
            },
            {
              "jobUrl": "http://localhost:4040/jobs/job?id=5",
              "$$hashKey": "object:3000"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851852752_25333577",
      "id": "paragraph_1634851852752_25333577",
      "dateCreated": "2021-10-21T21:30:52+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:2855",
      "dateFinished": "2021-10-21T21:32:23+0000",
      "dateStarted": "2021-10-21T21:32:10+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "the 1104\nof 691\nto 574\nor 500\nand 476\nOR 426\nOF 328\nTHE 322\nin 291\nany 258\n"
          }
        ]
      }
    },
    {
      "text": "%md\n\n## Ejecución de programas en Spark\n\n* En [Zeppelin](https://zeppelin.apache.org/) (como lo hacemos ahora)\n* En `pyspark` shell (tambien interactivo)\n* Como programa autónomo\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Ejecución de programas en Spark</h2>\n<ul>\n<li>En <a href=\"https://zeppelin.apache.org/\">Zeppelin</a> (como lo hacemos ahora)</li>\n<li>En <code>pyspark</code> shell (tambien interactivo)</li>\n<li>Como programa autónomo</li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632529_529954916",
      "id": "20171010-202757_196880209",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2174"
    },
    {
      "title": "pyspark shell",
      "text": "%md\n\n* Ir a la terminal donde corre [Zeppelin](https://zeppelin.apache.org/) en [Docker](https://www.docker.com/)\n* Oprimir [`ctrl`]-[`a`] y despues [`c`], para abrir otra terminal\n* Ir a la instalación Spark\n```sh\ncd /opt/spark\n```\n* Arrancar el shell\n```sh\n./bin/pyspark\n```\n* Escribir en shell (apretar `Enter` para ingresar cada línea)\n```python\n>>> lines = sc.textFile(\"README.md\")\n>>> lines.first()\n```\n* Para salir del shell oprima [`ctrl`]-[`d`]",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<ul>\n<li>Ir a la terminal donde corre <a href=\"https://zeppelin.apache.org/\">Zeppelin</a> en <a href=\"https://www.docker.com/\">Docker</a></li>\n<li>Oprimir [<code>ctrl</code>]-[<code>a</code>] y despues [<code>c</code>], para abrir otra terminal</li>\n<li>Ir a la instalación Spark</li>\n</ul>\n<pre><code class=\"language-sh\">cd /opt/spark\n</code></pre>\n<ul>\n<li>Arrancar el shell</li>\n</ul>\n<pre><code class=\"language-sh\">./bin/pyspark\n</code></pre>\n<ul>\n<li>Escribir en shell (apretar <code>Enter</code> para ingresar cada línea)</li>\n</ul>\n<pre><code class=\"language-python\">&gt;&gt;&gt; lines = sc.textFile(&quot;README.md&quot;)\n&gt;&gt;&gt; lines.first()\n</code></pre>\n<ul>\n<li>Para salir del shell oprima [<code>ctrl</code>]-[<code>d</code>]</li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632529_654789234",
      "id": "20171011-173126_528319238",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2175"
    },
    {
      "title": "Programa autónomo",
      "text": "%md\n\n* Ir a programa en repo git dentro de [Docker](https://www.docker.com/)\n```sh\ncd /diplodatos_bigdata/prog/word_count\n```\n* Ver programa\n```sh\nless src/main/python/WordCount.py\n```\n  (salir con [`q`])\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<ul>\n<li>Ir a programa en repo git dentro de <a href=\"https://www.docker.com/\">Docker</a></li>\n</ul>\n<pre><code class=\"language-sh\">cd /diplodatos_bigdata/prog/word_count\n</code></pre>\n<ul>\n<li>Ver programa</li>\n</ul>\n<pre><code class=\"language-sh\">less src/main/python/WordCount.py\n</code></pre>\n<p>(salir con [<code>q</code>])</p>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632529_252540873",
      "id": "20171011-175259_1199949339",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2176"
    },
    {
      "title": "Ejecucion de programa",
      "text": "%md\n* Ejecutar\n```sh\n/opt/spark/bin/spark-submit --master local[4] \\\n    src/main/python/WordCount.py /opt/spark/licenses/\n```\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<ul>\n<li>Ejecutar</li>\n</ul>\n<pre><code class=\"language-sh\">/opt/spark/bin/spark-submit --master local[4] \\\n    src/main/python/WordCount.py /opt/spark/licenses/\n</code></pre>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632530_1684574931",
      "id": "20171012-165049_905215351",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2177"
    },
    {
      "title": "Versión Spark en Zeppelin",
      "text": "%pyspark\n\nprint(sc.version)",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:36:07+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "fontSize": 14,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632530_749627758",
      "id": "20170830-114757_1684133948",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "FINISHED",
      "$$hashKey": "object:2178",
      "dateFinished": "2021-10-21T21:36:07+0000",
      "dateStarted": "2021-10-21T21:36:07+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "2.4.8\n"
          }
        ]
      }
    },
    {
      "text": "%md\n\n### Principales referencias online:\n\n* [Documentación Spark](https://spark.apache.org/docs/2.4.8/)\n* [API Spark Python](https://spark.apache.org/docs/2.4.8/api/python/index.html)\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h3>Principales referencias online:</h3>\n<ul>\n<li><a href=\"https://spark.apache.org/docs/2.4.8/\">Documentación Spark</a></li>\n<li><a href=\"https://spark.apache.org/docs/2.4.8/api/python/index.html\">API Spark Python</a></li>\n</ul>\n\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632530_1703285174",
      "id": "20181012-171203_1400816125",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2179"
    },
    {
      "text": "%md\n## Ejercicios MapReduce con Spark",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<h2>Ejercicios MapReduce con Spark</h2>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632531_1677746654",
      "id": "20171016-172908_1510165702",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2180"
    },
    {
      "title": "Ejercicio 1",
      "text": "%md\n\nModifique el programa *word count* siguiente para que cuente la **cantidad de apariciones de cada letra** en el archivo.\n\n* Ayuda: solo hay que modificar la linea 6\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Modifique el programa <em>word count</em> siguiente para que cuente la <strong>cantidad de apariciones de cada letra</strong> en el archivo.</p>\n<ul>\n  <li>Ayuda: solo hay que modificar la linea 6</li>\n</ul>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632531_20949842",
      "id": "20171010-202446_178633207",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2181"
    },
    {
      "text": "%pyspark\n\nlines = sc.textFile(\"README.md\")\n\nwords = lines \\\n    .flatMap(lambda line: [letter for letter in line.replace(\" \", \"\")]) \\\n    .filter(lambda word: word)\n\n\n#MapReduce\nwordCount = words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n\nresult = wordCount \\\n    .sortBy((lambda p: p[1]), ascending = False) # ordena por cantidad\n\nlocal_result = result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print(word, count) # los imprimo\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:37:45+0000",
      "progress": 0,
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 14,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://localhost:4040/jobs/job?id=6",
              "$$hashKey": "object:3136"
            },
            {
              "jobUrl": "http://localhost:4040/jobs/job?id=7",
              "$$hashKey": "object:3137"
            },
            {
              "jobUrl": "http://localhost:4040/jobs/job?id=8",
              "$$hashKey": "object:3138"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632531_1184343334",
      "id": "20201023-001957_322623490",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "FINISHED",
      "$$hashKey": "object:2182",
      "dateFinished": "2021-10-21T21:37:46+0000",
      "dateStarted": "2021-10-21T21:37:45+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "e 112\nt 98\na 85\ni 72\no 72\n/ 67\np 66\ns 62\nn 60\nr 58\n"
          }
        ]
      }
    },
    {
      "title": "Ejercicio 2",
      "text": "%md\nCada línea del archivo `~/diplodatos_bigdata/ds/links_raw.txt` contiene un url de una página web seguido de los links que posee a otras páginas web:\n```\n<url> <url link 1> <url link 2> ... <url link n>\n```\n\nBasándose en la utilización de la técnica de *MapReduce* que se mostró en el programa `word count` haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.\n\n#### Ayuda\n\nA continuación está el comienzo del programa. Falta hacer el *MapReduce* y mostrar el resultado.\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<div class=\"markdown-body\">\n<p>Cada línea del archivo <code>~/diplodatos_bigdata/ds/links_raw.txt</code> contiene un url de una página web seguido de los links que posee a otras páginas web:</p>\n<pre><code>&lt;url&gt; &lt;url link 1&gt; &lt;url link 2&gt; ... &lt;url link n&gt;\n</code></pre>\n<p>Basándose en la utilización de la técnica de <em>MapReduce</em> que se mostró en el programa <code>word count</code> haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.</p>\n<h4>Ayuda</h4>\n<p>A continuación está el comienzo del programa. Falta hacer el <em>MapReduce</em> y mostrar el resultado.</p>\n</div>"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632532_574677033",
      "id": "20171011-175322_1451259292",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2183"
    },
    {
      "text": "%pyspark\n\nbaseDir = \"/diplodatos_bigdata\" # llenar con el directorio git\n\nlines = sc.textFile(baseDir + \"/ds/links_raw.txt\")\n\nlinksTo = lines \\\n    .flatMap(lambda l: l.split(\" \")[1:]) # separo los links y tomo los apuntados\n\n# Ahora linksTo tiene las paginas apuntadas\n\n# Completar los ...\n\n# MapReduce\ninvLinkCount = linksTo.map(lambda link: (link,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n\nresult = invLinkCount.sortBy((lambda p: p[1]), ascending = False)\n\nlocal_result = result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print(word, count) # los imprimo\n",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:48:17+0000",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/python",
        "fontSize": 14,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://localhost:4040/jobs/job?id=9",
              "$$hashKey": "object:3302"
            },
            {
              "jobUrl": "http://localhost:4040/jobs/job?id=10",
              "$$hashKey": "object:3303"
            },
            {
              "jobUrl": "http://localhost:4040/jobs/job?id=11",
              "$$hashKey": "object:3304"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632533_1308629456",
      "id": "20191121-184701_1405603118",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "FINISHED",
      "$$hashKey": "object:2184",
      "dateFinished": "2021-10-21T21:48:18+0000",
      "dateStarted": "2021-10-21T21:48:17+0000",
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "http://www.yahoo.com/ 199\nhttp://www.ca.gov/ 169\nhttp://www.leginfo.ca.gov/calaw.html 155\nhttp://www.linkexchange.com/ 134\nhttp://www.berkeley.edu/ 126\nhttp://www.sen.ca.gov/ 123\nhttp://home.netscape.com/comprod/mirror/index.html 109\nhttp://www.assembly.ca.gov/ 99\nhttp://www.epa.gov/ 95\nhttp://www.usgs.gov/ 84\n"
          }
        ]
      }
    },
    {
      "title": "FIN",
      "text": "//val baseDir=\"https://git.cs.famaf.unc.edu.ar/dbarsotti/diplodatos_bigdata/raw/master/clases\"\nval baseDir=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases\"\n\nz.put(\"baseDir\", baseDir)\nprint(\"\"\"%html\n<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\".-\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\n\"\"\")",
      "user": "anonymous",
      "dateUpdated": "2021-10-21T21:27:12+0000",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "fontSize": 9,
        "title": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\".-\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\nbaseDir: String = https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1634851632533_1645565677",
      "id": "20160712-175904_2058049512",
      "dateCreated": "2021-10-21T21:27:12+0000",
      "status": "READY",
      "$$hashKey": "object:2185"
    }
  ],
  "name": "Clase 01 - Introducción a Spark",
  "id": "2GNFTUYPP",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/Clase 01 - Introducción a Spark"
}